To get up to date seed data you need to do three things and have a couple requirements. 
    1.  The ability to connect to our vpn
    2.  The database connection address and port number
    3.  A running docker container of our database
    4.  pg_dump, you can set this up in linux, windows, or mac.

The pg_dump command will pull in all of resources app in djangos tables as well as the synonym tables in the search app.  It pulls it in as a sql command inesert into column.  It will pull in some pg_audit stuff at the end that will cause an error but the rest of it will run fine and updates the tables as expected.  The next part is to clear out the resources tables and the synonym table which is done by an additional script.  The final step is to run the sql file created by pg_dump.

First run the pgdump command below.  Replace the db_name with the prod db address, and the port number for that port number.  Make sure you are also connected to the vpn in order to access AWS

pg_dump -h <db_address> -p <port_number> -U eregsuser -f backup.sql -t 'search_sy*' -t 'resources_*' --data-only --column-inserts eregs

The output for the backup.sql file will be wherever you ran the command from.  You can rename the backup.sql file if you wish but that is what its called in the command above.

Next run the clear-tables.sql file in this folder.  It will remove any currrent supplemental content, fr-docs, locations, synonyms files.  Does not touch regcore stuff or any admin tables.

Run the backup.sql file on your local database.  You can do this through command line or through a database tool.  Just make sure it has postgres capabilities.

To update our fixture data run the make command "make local.dump".  This will use django to create the json files for the seed data.

To test that the seed data is working properly run "make local.seed".  Everything should be working correctly once uploaded.